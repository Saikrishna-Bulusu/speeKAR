"""Speekar chatbot.ipynb
Original file is located at
    https://colab.research.google.com/drive/1GYmsZSR4MWuvORNpSWFWrXz79lQKb6oc
"""

import os

RESULTS_DIR = "scraped_files/"
os.makedirs(RESULTS_DIR, exist_ok=True)
import streamlit as st
import requests
import re
import urllib.request
from bs4 import BeautifulSoup
from collections import deque
from html.parser import HTMLParser
from urllib.parse import urlparse
import os
import pandas as pd
import numpy as np

# -*- coding: utf-8 -*-
"""SpeechIntegrated_KeywordAugmentedRetrieval_ChatBot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bbbci9ZbnAlXx-P7fb852dd9nWGEYGv9

# Keyword Augmented Retrieval



### Sample text
"""

#from keybert import KeyBERT
text = '''
 1.  Text Rank:

Experiments with several various co-occurrence window widths ranging from 2 to 10 were conducted in this technique, with 2 providing comparatively better results. In addition, to reduce noise, they use syntactic filters to remove just Nouns and Adjectives as probable candidate nodes when creating the graphs.

Once the graph is created, they use Page Rank until convergence to rank each node in the graph. The unweighted-undirected form of the pattern recognition algorithm is shown in the equation below —

Here, d is known as the Damping Factor (it is set to 0.85 to ensure that PR does not get trapped in graph cycles and may readily "teleport" to another node in the network). In(V) is the in-degree of node V, Out(V) is the out-degree of node V, and S(V) is the Page rank score for any given node. The graphic below depicts the Page Rank computation for a node in the graph using the previously given equation. It is worth noting that, since the graph is undirected, In-degree==Out-degree for every node in the graph.

Following convergence, each node in the network is assigned a numeric score that reflects its PageRank(PR) score. All of the keywords that initially exist as a neighbor in the real texts are combined to produce a single keyword as part of the post-processing stage to extract multi-word phrases as well.


2. Expand Rank:

In Expand Rank algorithm, we first generate a set of similar documents D for a given document to provide more knowledge and ultimately improve single document key extraction. The idea behind creating a similar document set is to allow the model to use global information in addition to the local information present in any given document. To find K-nearest neighbors, they use TF*IDF-based cosine similarity.

Following this step, they use a graph-based ranking algorithm to compute the global saliency score for each word in the word graph built on this expanded set. Because not all words in the documents are good indicators of keywords, certain syntactic filters are used during word graph construction. The edge weight between two words is calculated by multiplying the co-occurrence count of the two words across the entire document set by the similarity of the original document to the nearby concerned document, as shown in the equation:

Because this graph is based on the entire document set, it is known as the Global Affinity Graph. Once the ranking algorithm has reached a point of convergence, candidate keywords are merged to form a multi-word phrase. They use an additional rule to prune Adjective ending phrases and only select Noun ending phrases. A phrase's overall score is calculated by adding the saliency scores of individual words.

3. Position Rank:

The PageRank method for integrating the information about all the places of the word’s occurrence in a big text. The fundamental notion of PositionRank is to provide bigger weights (or probability) to words that are located early in a text compared to ones that appear in a later section of the document. Their algorithms primarily comprise three fundamental stages.

Graph creation at word level — They employ Nouns and Adjectives as candidates for creating nodes in their undirected word graph. Where edges connecting the nodes are based on a co-occurrence sliding window of a given size

Designing the Position-Biased PageRank — They weigh each proposed word with its opposite location in the text. For example — if a word is located in the following positions: 2nd, 5th, and 10th, then the weight associated with this word is 1/2 + 1/5 + 1/10 = 4/5 = 0.8. A vector is formed and set to the normalized weights for each potential word as shown below —


As of last, they apply the derived scores to Page Rank as mentioned above.

Formation of candidate phrases — Candidate words that have contiguous places in a text is concatenated to generate candidate phrases. They use another regex filter [(adjective)*(noun)+], of up to length three (i.e., unigrams, bigrams, and trigrams), on top of these candidate phrases to come up with the final list of keyphrases. Finally, the phrases are rated by adding up the scores of the words that compose the phrase. (Tip — You can also play with with “New Multi-word Keyword Scoring Strategy” stated above)

4. Yake:

A keyword extraction approach that employs statistical characteristics to discover and rank the most essential terms. It needs just a stopwords list for it to be language neutral. The complete algorithm has 4 stages to it —

* Preprocessing and Candidate term creation

They first do sentence-level split using segtok which is a rule-based sentence segmenter. Sentences are then separated into terms based on white space and other special characters (line break, comma, period) as the delimiters, and depending on the maximum length of keywords that we are interested in we may have 2, 3, 4 words split appropriately.

* Featurizing Terms

Here they lay down 5 qualities to rate every unit notably Casing(Tcase: More attention to capitalized and acronyms) (Tcase: More importance to capitalized and acronyms), Position of Word in the text (Tposition: More emphasis is given to the terms that are present at the beginning of the document), Word frequency (Tnorm), Term Relatedness to Context(Trel: Checks for the variety of context in which this word appears. The better the variety, the higher the possibilities for it becoming a popular term. It may be considered as a metric to prune regularly occurring terms like stop-words) and finally Term Various Sentence (Tsentence: This characteristic assesses how often a candidate word appears with different phrases. Higher score is given to terms that regularly appear in various phrases).

* Term Scoring

It utilizes the following algorithm to then compute the score for every unit.

* Deduplication

It is extremely feasible to acquire comparable morphological terms when graded according to the prior scoring method. To prevent redundancy they offer a Levenshtein distance-based deduplication strategy where the objective is not to pick a word if it has a short Levenshtein distance with previously chosen terms.

* Final Ranks

Minimum the scores, better the keywords. Pick top-k.

5. KeyBERT:

KeyBERT is a basic and easy-to-use keyword extraction approach that employs BERT embeddings to construct keywords and key phrases that are
most comparable to a text. Firstly, text embeddings are extracted using a pre-trained domain-specific BERT model. Secondly, word embeddings are then retrieved for N-gram words/phrases.
Finally, it employs cosine similarity as the similarity metric to discover the words/phrases that are most similar to the original material.

Top-k-rated words may then be regarded as the final collection of keyphrases.
Also, depending on the use case you can require a list of varied keywords for which you can utilize Maximal Marginal Relevance (MMR) or Max Sum Similarity.

\n
 '''+ '\n'
i=0
query="tell me important facts about pagerank"

line = text
n = 1500 #Number of characters to be included in a single chunk of text
text_split=[line[i:i+n] for i in range(0, len(line), n)]
print(text_split[0])

"""### Find headings for each para in a new document"""

from docx import Document
from docx.shared import Inches

document = Document("sample_data/rank_new.docx")
headings = []
para_texts = []
i=0
j=0
t=''
for paragraph in document.paragraphs:

    if paragraph.style.name == "Heading 2":
        no_free_text = "".join(filter(lambda x: not x.isdigit(), paragraph.text))
        k = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ";

        getVals = list(filter(lambda x: x in k, no_free_text))
        result = "".join(getVals)
        headings.append(result.lower())
        i=1
        #print(paragraph.text)

    elif paragraph.style.name == "normal" and i==1   :
        t+=paragraph.text


        try:
          if(document.paragraphs[j+1].style.name=="Heading 2"):
            i=0
            # to remove numeric digits from string

            para_texts.append(t)
            t=''
            #print(i)
        except:
          print("reached doc end")
          para_texts.append(t)
          #print(i)
    j+=1

print(len(para_texts), len(headings))
for h, t in zip(headings, para_texts):
    print(h, t)

all_text=''
for text in para_texts:
  all_text+=text

line = all_text
text_split=[line[i:i+n] for i in range(0, len(line), n)]
i=0
for t in  para_texts:
    print(i)
    print(t)
    i+=1

"""### Developing the chatbot function

Here, firstly a context is created and the chatbot function takes in the query responds based on a given context (as provided in "Rank_new.docx"). One can even change the document to suit their needs.
"""

import openai
from langchain.chat_models import ChatOpenAI
import os
from langchain.llms import OpenAI
from langchain import HuggingFacePipeline
from langchain.chains import RetrievalQA
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.chains import LLMChain
from dotenv import find_dotenv, load_dotenv
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)

from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.document_loaders import TextLoader
import textwrap
import glob

from keybert import KeyBERT
import time

kw_model = KeyBERT()


query="tell me important facts about pagerank"
query="tell me important facts about ExpandRank"
query="tell me important facts about position rank"
import pprint as pp

#print(headings)
def remove_newlines(serie):
    serie = serie.replace('\n', ' ')
    serie = serie.replace('\\n', ' ')
    serie = serie.replace('  ', ' ')
    serie = serie.replace('  ', ' ')
    return serie

text_split =[]
start_idx = 0
length = 768
end_idx = 0
print("len(all_text)::",len(all_text))
while end_idx < len(all_text):
            #print(f"end_idx:{end_idx} | len{len(text)}")
            end_idx = all_text.rfind(".", start_idx, length + start_idx) + 1
            #print(end_idx)
            text_split.append(all_text[start_idx:end_idx])
            start_idx = end_idx

def create_context(query, text_split):
    """
    Create a context for a question by finding the most similar context from the dataframe
    """

    # Get the embeddings for the question
    # Get the distances from the embeddings
    #i=0
    kw_model = KeyBERT()

    sentences = text_split #[i for i in nlp(text).sents]

    returns =[]
    keystart= time.time()
    keywords_q =[]


    keywords_query = kw_model.extract_keywords(query)
    keywords = []

    for j in range(len(keywords_query)):

          if(keywords_query[j][1]> 0.35 ):
            if( keywords_query[j][0] not in keywords_q):
              #print(keywords[j][0], keywords[j][1])
              keywords_q.append(keywords_query[j][0])


    i=0
    keyword_doc={}
    keyend= time.time()

    for sent in sentences:
      if(isinstance(sent, str) and len(sent)>6):
        keywords = kw_model.extract_keywords(sent)
        keyword_doc_sent=[]

        for j in range(len(keywords)):
            # Add the heading of  the para corresponding to the sentence
            if(j ==0):
              keyword_doc_sent.append(keywords[j][0])
              for h, pt in zip(headings, para_texts):

                      if(sent in pt):
                          keyword_doc_sent.append(h)
                          #print("sent ::",sent)
                          #print("heading::", h)
                          #print("para text ::",pt)
            if(keywords[j][1]> 0.35): # and keywords[j][0] not in keyword_doc ):
               if(keywords[j][0] not in keyword_doc_sent ):
                 keyword_doc_sent.append(keywords[j][0])
      keyword_doc[i]=keyword_doc_sent

      i+=1

    #print("doc keywords:: ",keyword_doc)
    #print("q keywords::",keywords_q)
    search_start= time.time()

    for i in range(len(keyword_doc)):
          for k in range(len(keywords_q)):
                  match_count= 0
                  if(keywords_q[k] in keyword_doc[i]):
                    match_count+=1
                    keywords.append(keywords_q[k])
                    #print(keywords_q[k],keyword_doc[i] )
                    #print("match_count::",match_count)
                    if( (match_count>=1 or match_count>=len(keywords_q)  ) ):
                      #print("Document matched :",i, "::")
                      if(remove_newlines(text_split[i]) not in returns ):
                        #context_q+=remove_newlines(sent)
                        returns.append(remove_newlines(text_split[i]))
                        #print(returns,match_count )


    searchend= time.time()
    search_time = searchend-search_start

    cur_len = 0


    # Return the context
    return "\n\n###\n\n".join(returns), keywords

messages =[]
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
import openai
import pprint as pp
import pandas as pd

import time

#Instead of calling the flan-t5 model as earlier, only the KAR based LLM responses are provided to converse real time.
def chatbot_slim(query, text_split):
    if input:

        stime= time.time()

        context, keywords = create_context(query, text_split)

        ctype=['stuff', 'map_reduce', 'refine', 'map_rerank']
        template= '''
              You are a helpful assistant who answers question based on context provided: {context}

              If you don't have enough information to answer the question, say: "Sorry, I cannot answer that".

              '''
        template= '''
                  You are a helpful assistant who answers question based on context provided: {context}

                  If you don't have enough information to answer the question, say: "I cannot answer".

                  '''
        template= ''' You answer question based on context below, and if question can't be answered based on context, say \"I don't know\"\n\nContext: {context} '''

        system_message_prompt= SystemMessagePromptTemplate.from_template(template)

        #Human question prompt

        human_template= 'Answer following question: {question}'

        template= ''' Answer question {question} based on context below, and if question can't be answered based on context,
        say \"I don't know\"\n\nContext: {context}

        Answer:
        '''

        template= ''' Use following pieces of context to answer the question. Provide answer in full detail using provided context.
        If you don't know the answer, say I don't know
        {context}
        Question : {question}
        Answer:'''


        human_message_prompt= HumanMessagePromptTemplate.from_template(human_template)

        chat_prompt= ChatPromptTemplate.from_messages(
            [system_message_prompt, human_message_prompt])

        chunk_size = 1024
        PROMPT = PromptTemplate(input_variables=["context", "question"],  template=template)

        chain_type_kwargs = {"prompt": PROMPT}

        question = query

        openai.api_key = "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"
        model="text-davinci-003"
        chat  = openai.Completion.create(
            prompt=f"You answer question based on context below, and if the question can't be answered based on the context, say \"I don't know\"\n\nContext: {context}\n\n---\n\nQuestion: {question}\nAnswer:",
            temperature=0,
            max_tokens=1000,
            top_p=1,
            frequency_penalty=0,
            presence_penalty=0,
            stop=None,
            model=model,
        )



        reply = chat["choices"][0]["text"].strip() # ['choices'][0]['message']['content']
        messages.append({"role": "assistant", "content": reply})

        return reply,context, keywords

"""## Text to speech and Speech recognition integration

First converted the designed queries into speech.Then convert the speech to text. Then use the available text to speech functions to read out the answers.
"""

from gtts import gTTS
import os, time
from IPython.display import Audio, display
import speech_recognition as sr
import soundfile as sf
from IPython.display import Audio, display


from pydub import AudioSegment

LANGUAGE = 'en'

QUERIES=["tell me important facts about expandrank", "tell me important facts about positionrank"," what is  positionrank",
   "what is page rank", "tell me important facts about pagerank", "tell me important facts about page rank"]



#timelist_query_readout = []
def texttospeech_raw(text,language,savename,slow=False):
  myobj = gTTS(text=text, lang=language, slow=False)

  # Saving the converted audio in a mp3 file
  myobj.save(savename+".mp3")

def texttospeech_nosave(text,language,savename,slow=False):
  myobj = gTTS(text=text, lang=language, slow=False)

  # Saving the converted audio in a mp3 file
  #myobj.save(savename+".mp3")

def texttospeech(text,language,savename,slow=False, autoplay=True):
  myobj = gTTS(text=text, lang=language, slow=False)

  # Saving the converted audio in a mp3 file
  myobj.save(savename+".mp3")

  # Playing the converted file
  #os.system("mpg321 keywords.mp3")

  #displaying the audio file interactively
  display(Audio(savename+".mp3", autoplay=True))

def mp3towav(input_file, output_file):
  # convert mp3 file to wav file
  sound = AudioSegment.from_mp3(input_file)
  sound.export(output_file, format="wav")



def speechtotext(query_audio):
  r = sr.Recognizer()

  audio_ex = sr.AudioFile(query_audio)
  type(audio_ex)

  # Create audio data
  with audio_ex as source:
      audiodata = r.record(audio_ex)
  type(audiodata)
  # Extract text
  text = r.recognize_google(audio_data=audiodata, language='en-US')
  return text

"""## Allowing Google colab to take voice input
Note that this might not be needed if one were to use a desktop/laptop/edge devide with a microphone and speaker inbuilt.
"""



!pip install ffmpeg-python
!pip install mutagen

from IPython.display import HTML, Audio
from google.colab.output import eval_js
from base64 import b64decode
import numpy as np
from scipy.io.wavfile import read as wav_read
import io
import ffmpeg

AUDIO_HTML = """
<script>
var my_div = document.createElement("DIV");
var my_p = document.createElement("P");
var my_btn = document.createElement("BUTTON");
var t = document.createTextNode("Press to start recording");

my_btn.appendChild(t);
//my_p.appendChild(my_btn);
my_div.appendChild(my_btn);
document.body.appendChild(my_div);

var base64data = 0;
var reader;
var recorder, gumStream;
var recordButton = my_btn;

var handleSuccess = function(stream) {
  gumStream = stream;
  var options = {
    //bitsPerSecond: 8000, //chrome seems to ignore, always 48k
    mimeType : 'audio/webm;codecs=opus'
    //mimeType : 'audio/webm;codecs=pcm'
  };
  //recorder = new MediaRecorder(stream, options);
  recorder = new MediaRecorder(stream);
  recorder.ondataavailable = function(e) {
    var url = URL.createObjectURL(e.data);
    var preview = document.createElement('audio');
    preview.controls = true;
    preview.src = url;
    document.body.appendChild(preview);

    reader = new FileReader();
    reader.readAsDataURL(e.data);
    reader.onloadend = function() {
      base64data = reader.result;
      //console.log("Inside FileReader:" + base64data);
    }
  };
  recorder.start();
  };

recordButton.innerText = "Recording... press to stop";

navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);


function toggleRecording() {
  if (recorder && recorder.state == "recording") {
      recorder.stop();
      gumStream.getAudioTracks()[0].stop();
      recordButton.innerText = "Saving the recording... pls wait!"
  }
}

// https://stackoverflow.com/a/951057
function sleep(ms) {
  return new Promise(resolve => setTimeout(resolve, ms));
}

var data = new Promise(resolve=>{
//recordButton.addEventListener("click", toggleRecording);
recordButton.onclick = ()=>{
toggleRecording()

sleep(2000).then(() => {
  // wait 2000ms for the data to be available...
  // ideally this should use something like await...
  //console.log("Inside data:" + base64data)
  resolve(base64data.toString())

});

}
});

</script>
"""

#This function allows google colab to access your microphone and record the Audio.
#This might lead to a slight increase in time which can be averted if done locally.
def get_audio():
  display(HTML(AUDIO_HTML))
  data = eval_js("data")
  binary = b64decode(data.split(',')[1])

  process = (ffmpeg
    .input('pipe:0')
    .output('pipe:1', format='wav')
    .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)
  )
  output, err = process.communicate(input=binary)

  riff_chunk_size = len(output) - 8
  # Break up the chunk size into four bytes, held in b.
  q = riff_chunk_size
  b = []
  for i in range(4):
      q, r = divmod(q, 256)
      b.append(r)

  # Replace bytes 4:8 in proc.stdout with the actual size of the RIFF chunk.
  riff = output[:4] + bytes(b) + output[8:]

  sr, audio = wav_read(io.BytesIO(riff))

  return audio, sr

#This is just to test the length of the MP3 file. Base don the length a waiting time has been added for the model to proceed and ask the user to record.
'''

audio = MP3("answer.mp3")
print(audio.info.length)'''

import scipy
import speech_recognition as sr

def speechtotextuserinput():
  audio, sample_rate = get_audio()
  scipy.io.wavfile.write('query.wav', sample_rate, audio)
  r = sr.Recognizer()
  audio_ex = sr.AudioFile("query.wav")
  type(audio_ex)

  # Create audio data
  with audio_ex as source:
    audiodata = r.record(audio_ex)
  type(audiodata)

  #audio = sr.AudioData(audio, sample_rate=24000, sample_width=4)
  text = r.recognize_google(audio_data = audiodata, language = "en-US")
  print(text)
  return text

import time, os

from pydub import AudioSegment
from pydub.playback import play
from mutagen.mp3 import MP3

#Here, a user's speech interface is considered and looped in conjunction with the KAR based speech response.

user_input = True
cnt = 0
while user_input==True:
  if cnt==0 or cnt>=2:
    user_input = input("Do you want to continue the conversation? (y/n)")
    if user_input == "y":
      user_input = True
    elif user_input == "n":
      user_input = False

  start = time.time()
  text = speechtotextuserinput()
  ans, context, keys = chatbot_slim(text, text_split)
  print(ans)
  texttospeech_raw(text=ans,language=LANGUAGE,savename="answer")
  audio = MP3("answer.mp3")
  display(Audio("answer.mp3", autoplay=True))
  time.sleep(audio.info.length)
  end = time.time()
  #print(end-start)
  cnt+=1

